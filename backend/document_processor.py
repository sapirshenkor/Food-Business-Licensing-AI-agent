"""
Comprehensive Document Processor for Business Licensing Requirements
Processes Hebrew regulatory documents with AI to extract all requirements accurately
"""

import anthropic
import docx
import json
import os
from datetime import datetime
from typing import Dict, List, Optional
from dotenv import load_dotenv

load_dotenv()

class ComprehensiveDocumentProcessor:
    def __init__(self):
        self.client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.usage_tracker={
            'total_calls': 0,
            'total_cost': 0.0,
            'input_tokens': 0,
            'output_tokens': 0
        }
    
    def process_document(self, file_path:str,output_path:str):
        """
        Main processing function - extracts all requirements from document
        
        Args:
            file_path: Path to Word document
            output_path: Where to save the processed requirements
            
        Returns:
            Dict: Complete requirements database
        """
        print("Starting comprehensive document processing...")

        # Step 1: Extract text from Word document
        print("ğŸ” Extracting text from Word document...")
        document_text = self._extract_text_from_word(file_path)
        if not document_text:
            raise Exception("Failed to extract text from Word document")
        
        print(f"âœ… Successfully extracted {len(document_text)} characters from Word document")
        print(f"ğŸ“Š Estimated tokens: {len(document_text.split()) * 1.3:.0f}")

        # Step 2: Process document with AI using comprehensive prompt
        print("ğŸ¤– Processing document with AI, this may take a few minutes...")
        requirements_data=self._process_with_ai(document_text)
        if not requirements_data:
            raise Exception("Failed to process document with AI")
        print("âœ… Successfully processed document with AI")

        # Step 3 validate and claen results
        print("ğŸ” Validating and cleaning extracted requirements...")
        validated_data=self._validate_and_clean_requirements(requirements_data)

        # Stage 4: save to json file
        print(f"ğŸ’¾ Saving validated requirements to JSON file {output_path}")
        self._save_to_json(validated_data, output_path)

        # Step 5: Print summary
        self._print_summary(validated_data)

        return validated_data
    
    def _extract_text_from_word(self, file_path:str):
        """
        Extracts text from a Word document using python-docx with structure preservation
        
        Args:
            file_path: Path to Word document
        """
        try:
            doc=docx.Document(file_path)
            full_text=""

            for paragraph in doc.paragraphs:
                text=paragraph.text.strip()
                if text:
                    # Adding structure markers based on paragraph style
                    style = paragraph.style.name.lower()
                    
                    if 'heading' in style or 'title' in style:
                        full_text += f"\n\n=== SECTION_HEADER: {text} ===\n"
                    elif paragraph.runs and paragraph.runs[0].bold:
                        full_text += f"\n--- SUBSECTION: {text} ---\n"
                    else:
                        full_text += f"{text}\n"
            
            # Clean and normalize
            full_text = self._clean_text(full_text)
            
            return full_text
            
        except Exception as e:
            print(f"âŒ Error extracting from Word: {e}")
            return None
    
    def _clean_text(self, text:str):
        """
        Cleans and normalizes text for better AI processing
        
        Args:
            text: Raw text to clean
        """

        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if line:
                cleaned_lines.append(line)
        
        # Join with single newlines
        cleaned_text = '\n'.join(cleaned_lines)
        
        # Remove BOM and other artifacts
        cleaned_text = cleaned_text.replace('\ufeff', '')
        return cleaned_text
    
    def _process_with_ai(self, document_text:str):
        """
        Processes document text with AI using comprehensive prompt.
        
        Args:
            document_text: Raw text to process
        """

        prompt = f"""
        ××ª×” ××¢×¨×›×ª AI ××ª×§×“××ª ×”××ª××—×” ×‘×¢×™×‘×•×“ ××¡××›×™× ×¨×’×•×œ×˜×•×¨×™×™× ×‘×¢×‘×¨×™×ª ×œ×¦×•×¨×š ×¨×™×©×•×™ ×¢×¡×§×™×.

        ×”××©×™××” ×©×œ×š: ×œ× ×ª×— ×‘×¦×•×¨×” ××§×™×¤×” ×•××“×•×™×§×ª ××ª ×”××¡××š ×”×‘× ×•×œ×—×œ×¥ ××ª ×›×œ ×”××™×“×¢ ×”×¨×œ×•×•× ×˜×™ ×œ×‘×¢×œ×™ ×¢×¡×§×™×.

        ×”××¡××š ×œ×¢×™×‘×•×“:
        {document_text}

        ×¢×œ×™×š ×œ×—×œ×¥ ××™×“×¢ ××•×‘× ×” ×•××¤×•×¨×˜ ×¢×œ:

        1. **×“×¨×™×©×•×ª ×›×œ×œ×™×•×ª ×œ×¢×¡×§×™×** - ×“×¨×™×©×•×ª ×©×—×œ×•×ª ×¢×œ ×›×œ ×¡×•×’×™ ×”×¢×¡×§×™×
        2. **×“×¨×™×©×•×ª ×¡×¤×¦×™×¤×™×•×ª ×œ×¤×™ ×’×•×“×œ** - ×“×¨×™×©×•×ª ×”××ª×™×™×—×¡×•×ª ×œ×’×•×“×œ ×”×¢×¡×§ ×‘×"×¨
        3. **×“×¨×™×©×•×ª ×¡×¤×¦×™×¤×™×•×ª ×œ×¤×™ ×ª×¤×•×¡×”** - ×“×¨×™×©×•×ª ×”××ª×™×™×—×¡×•×ª ×œ××¡×¤×¨ ×× ×©×™×
        4. **×“×¨×™×©×•×ª ×œ×××¤×™×™× ×™× ××™×•×—×“×™×** - ×’×–, ××©×œ×•×—×™×, ×”×’×©×ª ×‘×©×¨
        5. **×’×•×¤×™× ×¨×’×•×œ×˜×•×¨×™×™×** - ×›×œ ×”×¨×©×•×™×•×ª ×•×”××©×¨×“×™× ×”×¨×œ×•×•× ×˜×™×™×
        6. **×–×× ×™× ×•×¢×œ×•×™×•×ª** - ×œ×•×—×•×ª ×–×× ×™× ×•×¢×œ×•×™×•×ª ×œ×›×œ ×“×¨×™×©×”
        7. **×¤×¨×˜×™× ×—×©×•×‘×™× × ×•×¡×¤×™×** - ×›×œ ××™×“×¢ ×¨×œ×•×•× ×˜×™ ××—×¨

        ×¢×‘×•×¨ ×›×œ ×“×¨×™×©×”, ×¦×™×™×Ÿ ×‘×“×™×•×§:
        - ×©× ×”×“×¨×™×©×” ×”××“×•×™×§ (×›×¤×™ ×©××•×¤×™×¢ ×‘××¡××š)
        - ×”×’×•×£/×¨×©×•×ª ×”××•×¡××›×ª
        - ×ª×™××•×¨ ××¤×•×¨×˜ ×©×œ ×”×“×¨×™×©×”
        - ×ª× ××™ ×”×ª×¤×•×¡×” (××¡×¤×¨ ××§×•××•×ª/×× ×©×™× - ×× ×¨×œ×•×•× ×˜×™)
        - ×ª× ××™ ×©×˜×— (×’×•×“×œ ×‘×"×¨ - ×× ×¨×œ×•×•× ×˜×™)  
        - ×ª× ××™× ××™×•×—×“×™× (×©×™××•×© ×‘×’×–, ××©×œ×•×—×™×, ×”×’×©×ª ×‘×©×¨ - ×× ×¨×œ×•×•× ×˜×™)
        - ×–××Ÿ ×˜×™×¤×•×œ ××©×•×¢×¨
        - ×¢×œ×•×ª ××©×•×¢×¨×ª
        - ×¨××ª ×—×©×™×‘×•×ª/×¢×“×™×¤×•×ª
        - ×”×™×›×Ÿ ×‘××¡××š ×”×“×¨×™×©×” ××•×¤×™×¢×”
        - ×”×¢×¨×•×ª ××• ×ª× ××™× × ×•×¡×¤×™×
        - ×”×¢×¨×•×ª ×—×©×•×‘×•×ª 

        ×©×™× ×œ×‘ ××™×•×—×“ ×œ×’×•×¤×™× ×¨×’×•×œ×˜×•×¨×™×™× ×›××•:
        - ×“×¨×™×©×•×ª ×©×œ ××©×¨×“ ×”×‘×¨×™××•×ª
        - ×“×¨×™×©×•×ª ×›×‘××•×ª ×•×‘×˜×™×—×•×ª
        - ×¨×™×©×™×•× ×•×ª ×¢×¡×§ ×‘×¨×©×•×™×•×ª ××§×•××™×•×ª
        - ×“×¨×™×©×•×ª ×œ×¢×¡×§×™ ××–×•×Ÿ
        - ××™×©×•×¨×™ ×‘× ×™×™×” ×•×ª×›× ×•×Ÿ
        - ×“×¨×™×©×•×ª ×¡×‘×™×‘×ª×™×•×ª
        - ×“×¨×™×©×•×ª ×¢×‘×•×“×” ×•×‘×™×˜×—×•×Ÿ
        - ××™×¡×•×™ ×•×¨×™×©×•×

        ×”×—×–×¨ ×ª×©×•×‘×” ×‘×¤×•×¨××˜ JSON ×”×‘×:
        {{
        "document_analysis": {{
            "total_requirements_found": ××¡×¤×¨_×›×•×œ×œ,
            "document_sections": ["×¨×©×™××ª ×”×—×œ×§×™× ×”×¨××©×™×™× ×‘××¡××š"],
            "regulatory_authorities": ["×¨×©×™××ª ×›×œ ×”×’×•×¤×™× ×”××•×¡××›×™×"],
            "processing_notes": "×”×¢×¨×•×ª ×¢×œ ×ª×”×œ×™×š ×”×¢×™×‘×•×“",
            "document_length": ××¡×¤×¨_××™×œ×™×,
            "extraction_confidence": "×’×‘×•×”×”/×‘×™× ×•× ×™×ª/× ××•×›×”"
        }},
        "general_requirements": [
            {{
            "id": "general_001",
            "name": "×©× ×”×“×¨×™×©×”",
            "category": "×§×˜×’×•×¨×™×” ×›×œ×œ×™×ª",
            "authority": "×”×’×•×£ ×”××•×¡××š",
            "description": "×ª×™××•×¨ ××¤×•×¨×˜ ×©×œ ×”×“×¨×™×©×”",
            "applies_to": "×›×œ ×”×¢×¡×§×™×/×¢×¡×§×™× ××¡×•×’ ××¡×•×™×",
            "timeline": "×–××Ÿ ×˜×™×¤×•×œ",
            "estimated_cost": "×¢×œ×•×ª ××©×•×¢×¨×ª",
            "priority": "×’×‘×•×”×”/×‘×™× ×•× ×™×ª/× ××•×›×”",
            "source_location": "×”×™×›×Ÿ ×‘××¡××š",
            "additional_notes": "×”×¢×¨×•×ª × ×•×¡×¤×•×ª"
            }}
        ],
        "size_specific_requirements": [
            {{
            "id": "size_001",
            "name": "×©× ×”×“×¨×™×©×”",
            "category": "×“×¨×™×©×•×ª ×œ×¤×™ ×’×•×“×œ",
            "authority": "×”×’×•×£ ×”××•×¡××š",
            "description": "×ª×™××•×¨ ××¤×•×¨×˜",
            "conditions": {{
                "min_size_sqm": ××¡×¤×¨_××•_null,
                "max_size_sqm": ××¡×¤×¨_××•_null,
                "size_notes": "×”×¢×¨×•×ª ×¢×œ ×”×’×•×“×œ"
            }},
            "timeline": "×–××Ÿ ×˜×™×¤×•×œ",
            "estimated_cost": "×¢×œ×•×ª",
            "priority": "×¨××ª ×—×©×™×‘×•×ª",
            "source_location": "××™×§×•× ×‘××¡××š",
            "additional_notes": "×”×¢×¨×•×ª"
            }}
        ],
        "capacity_specific_requirements": [
            {{
            "id": "capacity_001",
            "name": "×©× ×”×“×¨×™×©×”",
            "category": "×“×¨×™×©×•×ª ×œ×¤×™ ×ª×¤×•×¡×”",
            "authority": "×”×’×•×£ ×”××•×¡××š",
            "description": "×ª×™××•×¨ ××¤×•×¨×˜",
            "conditions": {{
                "min_capacity": ××¡×¤×¨_××•_null,
                "max_capacity": ××¡×¤×¨_××•_null,
                "capacity_notes": "×”×¢×¨×•×ª ×¢×œ ×”×ª×¤×•×¡×”"
            }},
            "timeline": "×–××Ÿ ×˜×™×¤×•×œ",
            "estimated_cost": "×¢×œ×•×ª",
            "priority": "×¨××ª ×—×©×™×‘×•×ª",
            "source_location": "××™×§×•× ×‘××¡××š",
            "additional_notes": "×”×¢×¨×•×ª"
            }}
        ],
        "feature_specific_requirements": [
            {{
            "id": "feature_001",
            "name": "×©× ×”×“×¨×™×©×”",
            "category": "×“×¨×™×©×•×ª ×œ×¤×™ ×××¤×™×™× ×™×",
            "authority": "×”×’×•×£ ×”××•×¡××š",
            "description": "×ª×™××•×¨ ××¤×•×¨×˜",
            "conditions": {{
                "requires_gas": true/false/null,
                "has_delivery": true/false/null,
                "serves_meat": true/false/null,
                "feature_notes": "×”×¢×¨×•×ª ×¢×œ ×”×××¤×™×™× ×™×"
            }},
            "timeline": "×–××Ÿ ×˜×™×¤×•×œ",
            "estimated_cost": "×¢×œ×•×ª",
            "priority": "×¨××ª ×—×©×™×‘×•×ª",
            "source_location": "××™×§×•× ×‘××¡××š",
            "additional_notes": "×”×¢×¨×•×ª"
            }}
        ],
        "important_information": [
            {{
            "topic": "× ×•×©× ×—×©×•×‘",
            "description": "××™×“×¢ ×—×©×•×‘ ×©×œ× × ×›× ×¡ ×œ×§×˜×’×•×¨×™×•×ª ×”×§×•×“××•×ª",
            "relevance": "×œ××” ×–×” ×—×©×•×‘",
            "source_location": "××™×§×•× ×‘××¡××š"
            }}
        ]
        }}

        ×”×•×¨××•×ª ×—×©×•×‘×•×ª:
        1. ××œ ×ª××¦×™× ××™×“×¢ - ×¨×§ ××” ×©××•×¤×™×¢ ×‘××¡××š
        2. ×× ××©×”×• ×œ× ×‘×¨×•×¨, ×¦×™×™×Ÿ "×œ× ××•×’×“×¨" ××• "×“×•×¨×© ×‘×“×™×§×” × ×•×¡×¤×ª"
        3. ×©××•×¨ ×¢×œ ×“×™×•×§ ××§×¡×™××œ×™ - ×–×” ×™×™×•×©× ×‘××¢×¨×›×ª ×××™×ª×™×ª
        4. ×”×ª××§×“ ×‘×“×¨×™×©×•×ª ××¢×©×™×•×ª ×œ×‘×¢×œ×™ ×¢×¡×§×™× ×§×˜× ×™×-×‘×™× ×•× ×™×™×
        5. ×–×”×” ×§×©×¨×™× ×‘×™×Ÿ ×“×¨×™×©×•×ª ×©×•× ×•×ª
        6. ×©×™× ×œ×‘ ×œ×—×¨×™×’×™× ×•×ª× ××™× ××™×•×—×“×™×
        """
        try:
            print("ğŸ“¤ Sending document to Claude AI...")
            
            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=10000,
                messages=[{"role": "user", "content": prompt}]
            )
            
            # Track usage
            self._update_usage_tracker(response.usage)
            
            # Extract JSON response
            response_text = response.content[0].text
            requirements_data = self._extract_json_from_response(response_text)
            
            print("âœ… AI processing completed successfully")
            
            return requirements_data
            
        except Exception as e:
            print(f"âŒ AI processing error: {e}")
            return None
        
    def _extract_json_from_response(self, response_text:str):
        """
        Extracts JSON from AI response
        
        Args:
            response_text: Raw response text
        """
        try:
            # JSON boundaries without the start and end text that Claude adds
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = response_text[start_idx:end_idx]
                data = json.loads(json_str)
                return data
            else:
                raise ValueError("No JSON found in response")
                
        except json.JSONDecodeError as e:
            print(f"âŒ JSON parsing error: {e}")
            print("Raw response preview:", response_text[:500])
            
            # Save problematic response for debugging
            with open(f"debug_response_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt", 'w', encoding='utf-8') as f:
                f.write(response_text)
            
            return {
                "error": "JSON parsing failed",
                "raw_response": response_text
            }
    
    def _validate_and_clean_requirements(self, data:dict):
        """
        Validates and cleans extracted requirements
        
        Args:
            data: Raw requirements data
        """
        # Ensure required sections exist
        required_sections = [
            'document_analysis',
            'general_requirements', 
            'size_specific_requirements',
            'capacity_specific_requirements',
            'feature_specific_requirements'
        ]
        
        for section in required_sections:
            if section not in data:
                data[section] = []
                print(f"âš ï¸ Missing section: {section} - added empty list")
        
        # Add metadata
        data['processing_metadata'] = {
            'processed_at': datetime.now().isoformat(),
            'processor_version': '1.0.0',
            'api_calls_used': self.usage_tracker['total_calls'],
            'total_cost': round(self.usage_tracker['total_cost'], 4)
        }
        
        # Count total requirements
        total_reqs = (
            len(data.get('general_requirements', [])) +
            len(data.get('size_specific_requirements', [])) +
            len(data.get('capacity_specific_requirements', [])) +
            len(data.get('feature_specific_requirements', []))
        )
        
        data['summary'] = {
            'total_requirements': total_reqs,
            'general_requirements_count': len(data.get('general_requirements', [])),
            'size_specific_count': len(data.get('size_specific_requirements', [])),
            'capacity_specific_count': len(data.get('capacity_specific_requirements', [])),
            'feature_specific_count': len(data.get('feature_specific_requirements', []))
        }
        
        return data

    def _save_to_json(self, data:dict, output_path:str):
        """
        Saves data to a JSON file
        
        Args:
            data: Data to save
            output_path: Path to save the JSON file
        """
        try:
            # Create directory if needed
            os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
            
            # Save with pretty formatting
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… Data saved to {output_path}")
            
        except Exception as e:
            print(f"âŒ Error saving data: {e}")
            raise 
    
    def _print_summary(self, data:dict):
        """Print processing summary"""
        print("\n" + "="*60)
        print("ğŸ“Š PROCESSING SUMMARY")
        print("="*60)
        
        summary = data.get('summary', {})
        analysis = data.get('document_analysis', {})
        metadata = data.get('processing_metadata', {})
        
        print(f"ğŸ“„ Total Requirements Found: {summary.get('total_requirements', 0)}")
        print(f"   â€¢ General Requirements: {summary.get('general_requirements_count', 0)}")
        print(f"   â€¢ Size-Specific: {summary.get('size_specific_count', 0)}")
        print(f"   â€¢ Capacity-Specific: {summary.get('capacity_specific_count', 0)}")
        print(f"   â€¢ Feature-Specific: {summary.get('feature_specific_count', 0)}")
        
        if analysis:
            print(f"\nğŸ›ï¸ Regulatory Authorities Found: {len(analysis.get('regulatory_authorities', []))}")
            for auth in analysis.get('regulatory_authorities', [])[:5]:  # Show first 5
                print(f"   â€¢ {auth}")
        
        print(f"\nğŸ’° API Usage:")
        print(f"   â€¢ API Calls: {metadata.get('api_calls_used', 0)}")
        print(f"   â€¢ Total Cost: ${metadata.get('total_cost', 0):.4f}")
        print(f"   â€¢ Remaining Credits: ${5.0 - metadata.get('total_cost', 0):.4f}")
        
        print(f"\nâ° Processing Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        print("\nâœ… Document processing completed successfully!")
    
    def _update_usage_tracker(self, usage:dict):
        """
        Updates usage tracker with AI API usage
        
        Args:
            usage: Usage data from AI API
        """
        self.usage_tracker['total_calls'] += 1
        self.usage_tracker['input_tokens'] += usage.input_tokens
        self.usage_tracker['output_tokens'] += usage.output_tokens
        
        # Calculate cost (Claude 3.5 Sonnet pricing)
        input_cost = (usage.input_tokens / 1_000_000) * 3.0
        output_cost = (usage.output_tokens / 1_000_000) * 15.0
        call_cost = input_cost + output_cost
        
        self.usage_tracker['total_cost'] += call_cost
        
        print(f"ğŸ’¸ API Call Cost: ${call_cost:.4f}")
        print(f"ğŸ’° Total Cost So Far: ${self.usage_tracker['total_cost']:.4f}")
    
if __name__ == "__main__":
    # Initialize processor
    processor = ComprehensiveDocumentProcessor()
    
    # Process document
    document_path =os.getenv("DOCUMENT_PATH","regulatory_document.docx")
    output_path = os.getenv("OUTPUT_PATH","requirements.json")
    
    try:
        if not os.path.exists(document_path):
            print(f"âŒ Document not found: {document_path}")
            print("Please place your Word document in the same directory and update the path.")
        else:
            # Process the document
            results = processor.process_document(document_path, output_path)
            
    except Exception as e:
        print(f"âŒ Processing failed: {e}")
        print("\nTroubleshooting tips:")
        print("1. Check that your Word document exists")
        print("2. Verify your ANTHROPIC_API_KEY in .env file")
        print("3. Ensure you have sufficient API credits")